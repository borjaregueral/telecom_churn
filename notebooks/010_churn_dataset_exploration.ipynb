{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import churn.config as cfg\n",
    "from ydata_profiling import ProfileReport\n",
    "from churn.paths import create_directories, DATA_DIR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from churn.preprocessing import load_data\n",
    "from churn.plot import plot_barcharts, plot_boxplots\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from churn.analytics import (\n",
    "    cramers_v_for_unique_pairs, \n",
    "    correlation_matrix, \n",
    "    relationships_cat_vs_num, \n",
    "    analyze_features, \n",
    "    print_best_results, \n",
    "    fit_best_discretizers, \n",
    "    create_binned_dataset,\n",
    "    compare_variances\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the raw data\n",
    "create_directories()\n",
    "file_path = Path(DATA_DIR / 'churn.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "raw = load_data(file_path) \n",
    "# Display the first rows of the raw data\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profile report\n",
    "profile = ProfileReport(raw, title=\"Churn Dataset Report\")\n",
    "\n",
    "# Save the profile report to file\n",
    "path = Path(DATA_DIR / 'churn_dataset_report.html')\n",
    "profile.to_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column 'customer_hapiness' and apply the correct type to the variables\n",
    "raw = (raw\n",
    "       .rename(columns={'customer_hapiness': 'customer_happiness'})\n",
    "       .assign(\n",
    "    area_code=lambda df: df['area_code'].astype('category'),\n",
    "    plan=lambda df: df['plan'].astype('category'),\n",
    "    churn=lambda df: df['churn'].astype('category'),\n",
    "    total_day_minutes=lambda df: df['total_day_minutes'].round(),\n",
    "    total_day_calls=lambda df: df['total_day_calls'].round(),\n",
    "    total_day_charge=lambda df: df['total_day_charge'].round(2),\n",
    "    total_eve_minutes=lambda df: df['total_eve_minutes'].round(),\n",
    "    total_eve_calls=lambda df: df['total_eve_calls'].round(),\n",
    "    total_eve_charge=lambda df: df['total_eve_charge'].round(2),\n",
    "    total_night_minutes=lambda df: df['total_night_minutes'].round(),\n",
    "    total_night_calls=lambda df: df['total_night_calls'].round(),\n",
    "    total_night_charge=lambda df: df['total_night_charge'].round(2))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir los datos de partida en dos muestras entrenamiento y test\n",
    " -  Semilla: 123\n",
    " - Split para la muestra de test: 25%\n",
    " - División estratificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    raw.drop(columns='churn'), \n",
    "    raw['churn'], \n",
    "    test_size=cfg.TEST_SIZE, \n",
    "    random_state=cfg.SEED, \n",
    "    stratify=raw['churn']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and y_train for further analysis\n",
    "train_data = X_train.copy()\n",
    "train_data['churn'] = y_train.values\n",
    "\n",
    "# Combine X_test and y_test for future use\n",
    "test_data = X_test.copy()\n",
    "test_data['churn'] = y_test.values\n",
    "\n",
    "# Show the shapes of the resulting splits\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric variables\n",
    "numeric_data_train = train_data.select_dtypes(include=[np.number])\n",
    "# Identify categorical variables\n",
    "categorical_data_train = train_data.select_dtypes(include=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Estudiar la posible multicolinealidad entre las variables predictoras usando la correlación de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix of numeric variables\n",
    "correlation_matrix(numeric_data_train, 'pearson', cfg.FIG_SIZE,'coolwarm', True, '.3f', 8, False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Cramer's V for unique pairs of categorical variables\n",
    "cramer_v_categorical = cramers_v_for_unique_pairs(train_data)\n",
    "    \n",
    "# Show results\n",
    "cramer_v_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify relationships between categorical and numeric variables\n",
    "test_results_df = relationships_cat_vs_num(train_data, categorical_data_train, numeric_data_train)\n",
    "\n",
    "# Filter the DataFrame to only show significant relationships\n",
    "significant_results_df = test_results_df[test_results_df['P-Value'] < 0.05]\n",
    "\n",
    "# Show results\n",
    "significant_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Analizar de forma gráfica la distribución de las variables predictoras frente a la target (tanto numéricas como categóricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corregir los ejes para que indiquen lo que representan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots for numeric variables vs the target variable\n",
    "plot_boxplots(train_data, numeric_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical variables (including customer_service_rating) to plot\n",
    "variables_to_plot = train_data[['area_code','plan','customer_service_rating']]\n",
    "\n",
    "# Plor barcharts\n",
    "plot_barcharts(train_data, variables_to_plot, 'churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selección de características: entrenar un modelo random forest y escoger las 7 variables más importantes para el modelo\n",
    "\n",
    " Haz un entrenamiento simple con los siguientes hiperámetros:\n",
    "● número estimadores: 50\n",
    "● máxima profundidad del árbol: 6\n",
    "● semilla: 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest model with the specified hyperparameters\n",
    "rf_model = RandomForestClassifier(n_estimators=cfg.ESTIMATORS,\n",
    "                                  max_depth=cfg.DEPTH,\n",
    "                                  random_state=cfg.SEED\n",
    "                                  )\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain feature importances from the model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order and select only the top 7\n",
    "indices = np.argsort(importances)[-cfg.NUM_FEATURES:] \n",
    "\n",
    "# Select the features corresponding to the top 7 importances\n",
    "features = X_train.columns[indices]\n",
    "\n",
    "# Print the top 7 features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph showing the feature importance of the top 7 features\n",
    "fig = px.bar(\n",
    "    x=importances[indices],\n",
    "    y=features,\n",
    "    orientation='h',\n",
    "    labels={'x': 'Feature Importance', 'y': 'Features'},\n",
    "    color_discrete_sequence=['cornflowerblue']  # Set bar color to blue\n",
    ")\n",
    "\n",
    "# Update layout to make the chart smaller and set black background\n",
    "fig.update_layout(\n",
    "    height=400, \n",
    "    width=900, \n",
    "    title_text='Feature Importance (Random Forest)',\n",
    "    **cfg.PLOTLY_LAYOUT_CONFIG\n",
    ")\n",
    "\n",
    "# Plot the graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizar todas las variables predictoras.\n",
    "■ Cada variable debe contener, al menos, 4 niveles\n",
    "■ Utiliza cualquier técnica que consideres adecuada para tramear las variables numéricas (cuantiles, juicio experto, information-value)\n",
    "Estudiar el grado de asociación entre las variables predictoras (y frente al target). Puedes emplear la chi-square del paquete (spicy, R-básico) o la V-Cramer, su generalización.\n",
    "Ten en cuenta que el preprocesado debe hacerse, inicialmente, sobre la muestra de entrenamiento. Una vez definidos los niveles de las variables propuestas, estos deben ser también planteados en la muestra de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = X_train[features].copy()\n",
    "train_features['churn'] = y_train.values\n",
    "\n",
    "test_features = X_test[features].copy()\n",
    "test_features['churn'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the training features\n",
    "train_features = train_features.copy()\n",
    "\n",
    "# Establish the bin range from 4 to _\n",
    "bin_range = cfg.BIN_SIZES\n",
    "\n",
    "# Establish the optimization step\n",
    "opt_step = cfg.OPT_STEP\n",
    "\n",
    "# Analyze the features and find the best discretizers\n",
    "best_results = analyze_features(train_features, 'churn', bin_range, opt_step)\n",
    "\n",
    "# Print the best results\n",
    "print_best_results(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the variances of the original dataset with the binned dataset\n",
    "variance_comparison = compare_variances(best_results, train_features, 'total_eve_minutes')\n",
    "\n",
    "# Show the comparison\n",
    "display(variance_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best discretizers to the training features\n",
    "fitted_discretizers = fit_best_discretizers(train_features, best_results, 'churn')\n",
    "\n",
    "# Create the new binned training dataset\n",
    "train_features_binned = create_binned_dataset(train_features, fitted_discretizers, 'churn')\n",
    "\n",
    "# Display the new dataset with binned features\n",
    "train_features_binned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiar el grado de asociación entre las variables predictoras (y frente al target). Puedes emplear la chi-square del paquete (spicy, R-básico) o la V-Cramer, su generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate profile report for raw features (pre-binning)\n",
    "raw_features_report = ProfileReport(train_features, title=\"Raw_Feat\")\n",
    "\n",
    "# Generate profile report for binned features\n",
    "binned_features_report = ProfileReport(train_features_binned, title=\"Binned_Feat\")\n",
    "\n",
    "# Compare both datsets\n",
    "comparison_report = raw_features_report.compare(binned_features_report)\n",
    "\n",
    "# Save report to file\n",
    "path = Path(DATA_DIR / 'comparison_raw_binned_features.html')\n",
    "comparison_report.to_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to categorical\n",
    "train_features_binned = train_features_binned.astype('category')\n",
    "\n",
    "# Save to file the new binned dataset\n",
    "path = Path(DATA_DIR / 'train_features_binned.parquet')\n",
    "train_features_binned.to_parquet(path, index=False)\n",
    "\n",
    "# Calculate the Cramer's V for unique pairs of categorical variables\n",
    "cramer_v_features = cramers_v_for_unique_pairs(train_features_binned)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "features_comparison = pd.DataFrame(list(cramer_v_features.items()), columns=['Pair', 'Cramers_V'])\n",
    "\n",
    "# Sort the DataFrame by Cramer's V in descending order\n",
    "features_comparison = features_comparison.sort_values(by='Cramers_V', ascending=False)\n",
    "\n",
    "# Show the features comparison\n",
    "features_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the profile report to file\n",
    "path = Path(DATA_DIR / 'train_features_binned_report.html')\n",
    "binned_features_report.to_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only pairs with Cramér's V > 0.10\n",
    "features_comparison = features_comparison[features_comparison['Cramers_V'] > cfg.RELATIONSHIP_THRESHOLD]\n",
    "\n",
    "# Show results\n",
    "features_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el preprocesado debe hacerse, inicialmente, sobre la muestra de entrenamiento. Una vez definidos los niveles de las variables propuestas, estos deben ser también planteados en la muestra de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new binned training dataset\n",
    "test_features_binned = create_binned_dataset(test_features, fitted_discretizers, 'churn')\n",
    "\n",
    "# Convert all columns to categorical\n",
    "test_features_binned = test_features_binned.astype('category')\n",
    "\n",
    "# Save to file the new binned dataset\n",
    "path = Path(DATA_DIR / 'test_features_binned.parquet')\n",
    "test_features_binned.to_parquet(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn-c_H1yjx--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
